In this Jepsen post, we'll explore Zookeeper. Up next: NuoDB.

Zookeeper is a distributed CP datastore based on a consensus protocol called
ZAB. ZAB is similar to Paxos with Presidents in that it employs leaders and
majority quorums for commit, and offers strongly linearizable writes.

Because Zookeeper uses majority quorums, in an ensemble of five nodes, any two
can fail or be partitioned away without causing the system to halt. Any clients
connected to a majority component of the cluster can continue to make progress
safely. In addition, the linearizability property means that all clients will
see all updates in the same order--although clients may drift behind the
primary by an arbitrary duration.

This safety property comes at a cost: writes must be durably written to a disk
log on a majority of nodes before they are acknowledged. In addition, the
entire dataset must fit in memory. This means that Zookeeper is best deployed
for small pieces of state where linearizability and high availability is
critical. Often, ZK is used to track consistent *pointers* to larger, immutable
data stored in a different (perhaps AP) system; combining the safety and
scalability advantages of both (while simultaneously reducing the availability
for writes, since there are two systems to fail, and one of them is CP).

How does Zookeeper behave during partition?

In this test, five clients use a curator DistributedAtom to update a list of
numbers. The list is stored as a single serialized field, and updates are
applied via a CaS loop: atomically reading, decoding, appending the appropriate
number, enoding, and writing back iff the value has not changed.

```clj
(reify SetApp
  (setup [app]
    (reset!! state []))

  (add [app element]
    (try
      (swap!! state conj element)
      ok
      (catch org.apache.zookeeper.KeeperException$ConnectionLossException e
        error)))

  (results [app]
    @state)

  (teardown [app]
    (delete! curator path))))
```

Initially, the ZK leader is n1. During the test, we partition [n1 n2] away from
[n3 n4 n5], which means the leader cannot commit to a majority of nodes--and
consequently, writes immediately cease. After 15 seconds or so, a new leader is
elected in the majority component, and writes may proceed again. However, only
the clients which can see one of [n3 n4 n5] can write: clients connected to [n1
n2] time out while waiting to make contact with the leader:

[[image:zk.png]]

When the partition is resolved, writes on [n1 n2] begin to succeed right away;
the leader election protocol is stable, so there is no need for a second
transition during recovery.

Consequently, in a short test (~200 seconds, ~70 second partition, evenly
distributed constant write load across all nodes) ZK might offer 78%
availability, asymptotically converging on 60% (3/5 nodes) availability as the
duration of the partition lengthens. ZK has never dropped an acknowledged write
in any Jepsen test, and typically yields 0-2 false positives, likely due to
writes proxied through n1 and n2 just prior to the partition--such that the
write committed, but the acknowledgement was not received by the proxying node.

Keep in mind that, like as with all experiments, we can only disconfirm hypotheses. This test demonstrates that in the presence of a partition and leader election, Zookeeper is able to maintain the linearizability invariant. However, there could be other failure loads or write patterns which lead to data loss. Nevertheless, this is a heartening result; one that all CP datastores should aspire to produce.

In summary, *use Zookeeper*. It's mature, well-designed, and battle-tested.
Because the consequences of its connection model and linearizability properties
are subtle, you should, wherever possible, take advantage of tested recipes and
client libraries like Curator, which do their best to correctly handle the
complex state transitions associated with session and connection loss.

Also keep in mind that linearizable state in Zookeeper (such as leader
election) does not guarantee the linearizability of a downstream system. For
instance, a cluster which uses ZK to pick a single leader might allow multiple
nodes to be the leader simultaneously. Even if there are no simultaneous
leaders at the same wall-clock time, message delays can result in logical
inconsistencies. Designing CP systems, even with a strong coordinator, requires
carefully coupling the operations in the system to the underlying coordinator
state.
