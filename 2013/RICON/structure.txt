Introduce self
  - Kyle Kingsbury
  - Work at Factual
  - This will be a pragmatic talk
    - What happens when theory meets hardware

Interested in the problem of *reliability*
  - Can mean consistency
  - Can mean availability
  - CAP puts complex, balanced limits on these
  - A key prerequisite: know what happened to your operation
  - Goal of many applications: time-bounded consistency
    - I want to tell the user that their write succeeded, or failed, in 5 s
    - "Succeeded" can be flexible
      - Dear user, you'll read your writes now
      - Dear user, I promise this write is causally connected to some future
        state of the system
      - Dear user, YOLO

Introduce partitions
  - What are partitions?
    - A communication breakdown
    - "unusually high" latency between nodes
    - Practically speaking: high enough that timeouts come into play

Introduce workload
  - Write N distinct integers to a list
  - 5 clients
  - Each client has its own disjoint set of writes
  - Concurrent writes
    - Use atomic operations
  - Going to express this application in several different databases
  - This is not a real-world workload
    - Simple to explain, write, and verify
    - It's a branching-off point to demonstrate different types of behavior

Introduce cluster
  - 5 nodes
  - 5 servers
  - The primary, where applicable, lives on n1
  - We're going to split the cluster into two components
    - DIAGRAM
    - Minority component: n1, n2
    - Majority component: n3, n4, n5


Postgres
  - Postgres is a classic ACID relational database
    - Strong consistency model: 1 primary.
    - If single-node, CP
    - If you use synchronous primary->secondary replication, still CP, durable
    - If you use asynchronous replication, and never promote secondary, CP,
      less durable
    - Secondaries can not accept writes
  - Postgres is consistent
  - But even though Postgres itself is consistent, client+server is not
    - Possible for clients and servers to disagree about a commit
    - How?
      - Postgres uses 2PC
        - When transaction is finished, the coordinator (client) votes to commit
        - PG checks to see if it's OK, and commits
        - PG tells the client that the commit succeeded
      - Client and server now agree that the commit took place
      - What if the last message is dropped?
        - Client *doesn't* know whether PG committed or not
        - As a developer, network failure usually looks like an error
        - Assume the transaction failed
  - DEMO
    - To start, we'll use a single server
    - DIAGRAM: 1 server, 5 clients
    - Clients will each try to insert a row within a transaction
    - But now, I'm going to cut the clients off from the server
    - DIAGRAM: Server cut off from clients
    - Observe failure
    - Heal
    - The app encountered exceptions during these N transactions
    - But they actually committed!
  - Strategies
    - Read/write errors mean "I don't know", not "it failed"
    - Ignore inconsistency
    - Make operations idempotent and retry
    - Ask for current transaction ID and write it to a table, then check

Redis
  - Redis is a data structure server
  - Not designed for distribution: single primary server, n secondaries
    - Asynchronous replication only
  - How do you provide high availability when the primary is unreachable?
    - Promote a secondary node
    - Lose N seconds of writes depending on replication log delay
      - In line with Redis's disk persistence: lose up to n seconds on crash
      - Not a durable datastore
    - Redis Sentinel
      - Sentinels observe Redis servers and vote on up/down state
      - Promote slave when sentinels have quorum
        - Make sure you choose a quorum value bigger than N/2!
      - Sentinels use pubsub to broadcast the current primary node to clients
  - DEMO
    - DIAGRAM: 5 clients, 5 servers, 5 sentinels
    - DIAGRAM: partition nodes
    - Majority sentinels should detect loss of primary and agree to promote
    - Start app
    - Partition, watch for re-election in logs
    - Show clients switching to new master
    - Oh snap: the old master never steps down
    - Split brain during the partition: two inconsistent servers
    - Heal partition
    - Does a primary step down?
    - We've got two inconsistent sets of writes
    - Destroy all writes on one of the servers
  - Implications
    - It's actually worse than that
    - Even if the old primary did step down, there's no guarantee of *ordering*
    - In a single-primary CP system there must *only* be one primary at a time
    - Old primary must step down before we can elect a new one
    - No guarantee that this happens!
      - "What's going on over there anyway?"
    - Asynchronous replication means servers *will* drop acknowledged writes
  - Strategies
    - Redis is not a database
    - HA redis is not consistent
    - HA redis is certainly not a distributed lock server
    - Ensure you can tolerate/recover from any number of dropped writes
      - Or inconsistent views of the world

Mongo
  - Mongodb is a document store
    - Resilient to node failures
      - Mirrors documents to all members of a replica set
    - Replica sets have a single primary, n secondaries
      - CP-ish: minority component should go unavailable
      - Asynchronous replication
      - Write to primary, read from all nodes
        - Means you can fail to read your writes
        - Use ReadConcern Primary
        - Turns out getting monotonic consistency is hard
    - Like Redis, quorum-based primary election
      - But handled by DB nodes themselves, not sentinels
  - DEMO: unacknowledged (old default)
    - DIAGRAM: old defaults of error ignored
    - DIAGRAM: partition, oplog cut off
    - Drops N % of writes
    - Why was this ever the default?
  - DEMO: acknowledged (new default)
    - DIAGRAM: Writes to 1 before returning
    - DIAGRAM: Partition, isolates oplog
    - Why are we losing writes?
    - Because we haven't guaranteed that at least 1 member of the majority
      component will have our write
      - DIAGRAM: Divergent histories
      - Mongo's conflict resolution strategy is to pick an arbitrary node
        and drop its writes on the floor
        - Rollback file. Maybe.
        - Have fun merging
  - Replication is asynchronous
    - But write concerns allow you to block until replicated to N
    - Means that in theory, we can achieve consistency by writing to majority
  - DEMO: MAJORITY
    - DIAGRAM: a node in the majority component receives write
    - DIAGRAM: partitioned, write will be visible in new primary
    - Slow!
    - Still drops writes
      - WTF?
      - It's a Mongo bug which treats conn failures as acknowledgements
  - OK, what if the bug didn't exist
    - False negatives
      - Writes to replica sets are not atomic
      - If it succeeds on primary, but doesn't get replicated, still on disk
      - Will be readable for... a while. Maybe forever.
      - What if you write an empty value?
        - Could clobber data on other nodes when rejoined
    - Interesting problems with asymmetric partitions
    - Clock skew can still lead to independent primaries
      - Consequeneces for trying to achieve monotonic consistency
  - Strategies
    - Accept data loss
    - Write with MAJORITY
    - Write a merge function and use rollbacks?

Riak
  - Eventually consistent distributed KV store
  - "Tunable CAP controls"
    - DIAGRAMS: r/w tuning
    - In practice, varying degrees of AP
    - Asynchronous replication
    - Strong CP guarantees may be coming
    - False negatives: writes which fail can still succeed and eventually
      be replicated
      - Root of all evil?
  - When two people write to the same value simultaneously, keeps *both* values
    - Defaults: allow_mult=false
    - Resolve conflicting versions by picking the one with the most recent time
    - Like Mongo, only for each object individually
  - DEMO: LWW, R=W=ALL
    - DIAGRAM
    - Data loss!
    - Why? Simultaneous writes are clobbering each other
  - DEMO: introduce a global lock
    - No data loss
    - Guaranteed to not step on each other
  - DEMO: global lock + partition
    - Data loss
    - Even with R=W=ALL
    - Even with perfect distributed global lock
    - Sloppy quorum problem
      - DIAGRAM: riak establishes whole ring on each side
      - When partition resolves, discards one history
  - DEMO: PR=PW=ALL
    - DIAGRAM
    - Works, I think. Mostly.
  - What if we wanted to be available everywhere?
    - CRDTs
    - A datatype with a merge function which is
      - Associative
      - Commutative
      - Idempotent
    - DIAGRAM: merging histories
    - If you use CRDTs, partition tolerance is provable
  - DEMO: CRDTs
    - 100% success
    - Note false negatives
    - Slow, large, GC problems
  - Strategies:
    - Accept data loss
    - Never LWW
      - Even a dumb merge function is preferable
      - Chances are you can save *some* of your data

Recap
  - Postgres: strong transactional consistency
    - Client can be inconsistent
    - A very minor problem for most apps
      - Easy to recover CP at the client
  - MySQL, Oracle, MSSQL, etc
    - Varying degrees of CP ACID designs
    - Similar transactional semantics
  - Redis: single-primary design which allows multiple primaries
    - Enters split-brain readily
    - Must deal with divergent histories
    - Can't really work around this problem
  - Mongo: single-primary design which allows multiple primaries
    - In general, this is a dangerous property
    - Must deal with divergent histories
    - Can get close to CP for selected writes
  - Riak: AP design
    - Trying to achieve CP is dangerous
    - Writes are not transactional
    - Shines as an AP database
    - Siblings really should be mandatory
      - Except where you absolutely know the correct current state of an obj
      - Writing *changes*: use CRDTs
  - Cassandra: also a dynamo ring
    - Traditionally LWW
      - Subject to similar data-loss problems under partition
    - Now working on Paxos-backed compare-and-set operations
      - Could allow CP transactions when quorum available?

- Big questions
  - How do you know your app is *really* consistent?
  - Implementation (and bugs!) can trump theory
    - Postgres: pretty solid, IMO
    - Redis: not even sure what it's supposed to do
    - Mongo: makes claims, but they don't work
    - Riak: partitions can push latencies to extreme heights
  - Need to test the DB config, datatypes, and client together
    - Lots of integration, but not spectacularly difficult
  - As databases grow, you start to find more and more inconsistencies
    - Consider behavior under partition as a possible cause
    - Lots of things look like partitions
  - Consider simplicity over safety
    - Drop/GC inconsistent writes
    - Little white lies
    - Performance can trump correctness
    - Statistical bounds
    - For more, see Camille Fournier's talk
